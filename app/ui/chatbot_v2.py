# -*- coding: utf-8 -*-
# @Time    : 2025/06/18 3:31â€¯AM
# @Author  : Galleons
# @File    : chatbot_v2.py

"""
ç¬¬äºŒç‰ˆæ¨ç†å¯è§†åŒ– demo å±•ç¤ºç•Œé¢ï¼Œæ•´åˆæ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
"""

import gradio as gr
from gradio import ChatMessage

import time
import os
import uuid

from app.core.mq import publish_to_rabbitmq
from app.core.config import settings
from app.core import logger_utils
from app.pipeline.feature_pipeline.models.raw import DocumentRawModel
from app.pipeline.inference_pipeline.reasoning import ReasoningPipeline
from qdrant_client import QdrantClient, models
from app.pipeline.inference_pipeline.prompt_templates import InferenceTemplate
from app.core.rag.retriever import VectorRetriever
from app.core.rag.prompt_templates import QueryExpansionTemplate
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from app.pipeline.inference_pipeline.utils import compute_num_tokens, truncate_text_to_max_tokens
from pathlib import Path
from markitdown import MarkItDown


ROOT_DIR = str(Path(__file__).parent.parent.parent.parent)
UPLOAD_FOLDER = os.path.join(ROOT_DIR, "uploads")


sleep_time = 0.5
model = ChatOpenAI(model=settings.MODEL_PATH,
                   api_key=settings.KEY,
                   base_url=settings.LOCAL,
                   extra_body={"chat_template_kwargs": {"enable_thinking": False}},)
query_expansion_template = QueryExpansionTemplate()
prompt = query_expansion_template.create_template(3)
chain = prompt | model
client = QdrantClient(url="http://localhost:6333")
doc_bases = [collection.name for collection in client.get_collections().collections]

logger = logger_utils.get_logger(__name__)


def process_uploaded_file(files: list, dir_files: list, collection_choice: str = 'default'):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæ”¯æŒå•ä¸ªæ–‡ä»¶ä¸Šä¼ å’Œç›®å½•ä¸Šä¼ ä¸¤ç§æ–¹å¼ã€‚

    å‚æ•°:
        files (list): å•ä¸ªæ–‡ä»¶ä¸Šä¼ åˆ—è¡¨
        dir_files (list): ç›®å½•ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
        collection_choice (str): ä¸Šä¼ çš„çŸ¥è¯†åº“å

    è¿”å›:
        tuple: (çŠ¶æ€æ¶ˆæ¯, æ–‡ä»¶å, æ–‡ä»¶å)
    """
    try:
        # è¿‡æ»¤æ‰Noneå€¼å’Œç©ºåˆ—è¡¨
        files = [f for f in files if f] if files else []
        dir_files = [f for f in dir_files if f] if dir_files else []

        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶
        files_list = files + dir_files

        if not files_list:
            return "æœªé€‰æ‹©ä»»ä½•æ–‡ä»¶", "", ""

        doc_count = len(files_list)
        logger.info(f"å…±æ”¶åˆ°{doc_count}ä¸ªæ–‡ä»¶ï¼Œæ–‡ä»¶åï¼š{files_list}")

        md = MarkItDown(enable_plugins=False)

        processed_count = 0
        for file in files_list:
            try:
                result = md.convert(file)
                data = DocumentRawModel(
                    knowledge_id=collection_choice,
                    doc_id="222",
                    path=file,
                    filename=file,
                    content=result.text_content,
                    type="documents",
                    entry_id=str(uuid.uuid4()),
                ).model_dump_json()
                publish_to_rabbitmq(queue_name='test_files', data=data)
                logger.info(f"æˆåŠŸå¤„ç†å¹¶å‘é€æ–‡ä»¶ï¼š{file}")
                processed_count += 1
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {file} æ—¶å‡ºé”™: {str(e)}")
                continue

        if processed_count == 0:
            return "æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®", "", ""

        return f"æˆåŠŸå¤„ç† {processed_count}/{doc_count} ä¸ªæ–‡ä»¶", file_name, file_name

    except Exception as e:
        logger.error(f"å¤„ç†ä¸Šä¼ æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", "", ""


def process_query(query: str, show_reasoning: bool = False, use_background: bool = True,
                  selected_collections: list = None):
    inference_endpoint = ReasoningPipeline(mock=False)

    response = inference_endpoint.generate(
        query=query,
        enable_rag=True,
        sample_for_evaluation=True,
        doc_names=selected_collections if selected_collections else None
    )

    return response['answer']


def add_new_collection(new_collection: str):
    """
    æ·»åŠ æ–°çš„çŸ¥è¯†åº“é›†åˆ

    å‚æ•°:
        new_collection (str): æ–°çŸ¥è¯†åº“åç§°
        current_collections (list): å½“å‰çŸ¥è¯†åº“åˆ—è¡¨

    è¿”å›:
        tuple: (æ›´æ–°åçš„çŸ¥è¯†åº“åˆ—è¡¨, æ–°çŸ¥è¯†åº“åç§°)
    """
    global doc_bases
    if not new_collection or new_collection.strip() == "":
        return doc_bases

    if new_collection in doc_bases:
        return doc_bases

    client.create_collection(
        collection_name=new_collection,
        vectors_config=models.VectorParams(size=settings.EMBEDDING_SIZE, distance=models.Distance.COSINE),
        quantization_config=models.ScalarQuantization(
            scalar=models.ScalarQuantizationConfig(type=models.ScalarType.INT8, quantile=0.99, always_ram=True, ), ),
    )

    logger.debug(f"{doc_bases}<UNK>{new_collection}<UNK>]")
    doc_bases.append(new_collection)

    return doc_bases



def format_prompt(
        system_prompt,
        prompt_template: PromptTemplate,
        prompt_template_variables: dict,
) -> tuple[list[dict[str, str]], int]:
    prompt = prompt_template.format(**prompt_template_variables)

    num_system_prompt_tokens = compute_num_tokens(system_prompt)
    prompt, prompt_num_tokens = truncate_text_to_max_tokens(
        prompt, max_tokens=settings.MAX_INPUT_TOKENS - num_system_prompt_tokens
    )
    total_input_tokens = num_system_prompt_tokens + prompt_num_tokens

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt},
    ]

    return messages, total_input_tokens

def kunlunrag_thinking_chat(prompt: str, history: list):
    history.append(ChatMessage(role="user", content=prompt))
    yield history

    start_time = time.time()
    history.append(ChatMessage(
        content="",
        metadata={"title": "_ç”Ÿæˆå¤šé‡æŸ¥è¯¢_", "id": 0, "status": "pending"}
    ))
    yield history
    prompt_template_builder = InferenceTemplate()
    system_prompt, prompt_template = prompt_template_builder.create_template(enable_rag=True)
    prompt_template_variables = {"question": prompt}

    retriever = VectorRetriever(query=prompt)

    full_output = ''
    for chunk in chain.stream({"question": prompt}):
        # print(chunk, end="|", flush=True)
        full_output += chunk.content
        history[-1].content = full_output.strip()
        yield history


    queries = full_output.strip().split(query_expansion_template.separator)
    stripped_queries = [
        stripped_item for item in queries if (stripped_item := item.strip(" \\n"))
    ]
    logger.debug(stripped_queries)
    hits = retriever.retrieve_top_k(
        k=3, collections=['zsk_demo'], generated_queries=stripped_queries
    )


    history.append(ChatMessage(
        content="",
        metadata={"title": "æŸ¥è¯¢åˆ°ç›¸å…³æ–‡æ¡£", "id": 1, "status": "pending"}
    ))
    history[-1].metadata["duration"] = time.time() - start_time
    accumulated_thoughts = ""
    for hit in hits:
        time.sleep(sleep_time)
        accumulated_thoughts += f"- {hit}\n\n"
        history[-1].content = accumulated_thoughts.strip()
        yield history

    history.append(ChatMessage(
        content="",
        metadata={"title": "å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’", "id": 2, "status": "pending"}
    ))
    context = retriever.rerank(hits=hits, keep_top_k=3)
    prompt_template_variables["context"] = context
    messages, input_num_tokens = format_prompt(
        system_prompt, prompt_template, prompt_template_variables
    )
    yield history

    history[-1].metadata["status"] = "done"
    history[-1].metadata["duration"] = time.time() - start_time
    yield history

    from openai import OpenAI

    client = OpenAI(api_key="sk-jkcrphotzrjcdttdpbdzczufqryzmeogzbvwbtpabuitgnzx",
                    base_url="https://api.siliconflow.cn/v1")
    answer = client.chat.completions.create(
        # model='Pro/deepseek-ai/DeepSeek-R1',
        model="deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
        messages=messages,
        stream=True,
        max_tokens=1000,
    )

    thought_buffer = ""
    response_buffer = ""
    thinking_complete = False

    history.append(
        ChatMessage(
            role="assistant",
            content="",
            metadata={"title": "â³Thinking: *æ­£åœ¨æ€è€ƒ"}
        )
    )


    for chunk in answer:
        content = chunk.choices[0].delta.content
        reasoning_content = chunk.choices[0].delta.reasoning_content

        if content and not thinking_complete:
            # Complete thought and start response
            content += content or ""
            history[-1] = ChatMessage(
                role="assistant",
                content=thought_buffer,
                metadata={"title": "â³Thinking: *æ­£åœ¨æ€è€ƒ"}
            )

            # Add response message
            history.append(
                ChatMessage(
                    role="assistant",
                    content=content
                )
            )
            response_buffer += content
            thinking_complete = True


        elif thinking_complete:
            # Continue streaming response
            response_buffer += content or ""
            history[-1] = ChatMessage(
                role="assistant",
                content=response_buffer,
                #metadata = {"title": "æœ€ç»ˆå›ç­”ï¼"}
            )

        else:
            # Continue streaming thoughts
            thought_buffer += reasoning_content or ""
            history[-1] = ChatMessage(
                role="assistant",
                content=thought_buffer,
                metadata={"title": "â³Thinking: *æ¨ç†æ€ç»´é“¾"}
            )

        yield history

with gr.Blocks(title="æ¨ç†é—®ç­”çŸ¥è¯†åº“") as demo:
    gr.Markdown("""
    # æ¨ç†é—®ç­”ç³»ç»Ÿ
        æ”¯æŒå¤šæ–‡ä»¶ä¸Šä¼ ï¼Œå¤§æ–‡æ¡£ä¸Šä¼ 
        æ”¯æŒå¤šæ–‡ä»¶å¤¹ä¸Šä¼ ï¼Œå¤§æ–‡ä»¶å¤¹ä¸Šä¼ 
    """)

    with gr.Row():
        with gr.Column():
            files_input = gr.File(
                label="ä¸Šä¼ æ–‡æ¡£",
                file_types=[".txt", ".docx", ".pdf", ".csv", ".json"],
                type="filepath",
                file_count="multiple"
            )
            dir_input = gr.File(
                label="ä¸Šä¼ æ–‡ä»¶å¤¹",
                file_types=[".txt", ".docx", ".pdf", ".csv", ".json"],
                type="filepath",
                file_count="directory"
            )

            with gr.Row():
                collection_choice = gr.Dropdown(
                    label="é€‰æ‹©åŠ è½½åˆ°çš„çŸ¥è¯†åº“",
                    choices=doc_bases,
                    multiselect=False,
                    value=doc_bases[0] if doc_bases else None,
                    interactive=True
                )
                new_collection_input = gr.Textbox(
                    label="æ–°å»ºçŸ¥è¯†åº“",
                    placeholder="è¾“å…¥æ–°çŸ¥è¯†åº“åç§°",
                    interactive=True
                )
                add_collection_btn = gr.Button("æ·»åŠ ")

            file_name = gr.Textbox(label="æ–‡ä»¶å", visible=False)

            upload_button = gr.Button("åŠ è½½æ–‡æ¡£åˆ°çŸ¥è¯†åº“")

            upload_output = gr.Textbox(label="ä¸Šä¼ çŠ¶æ€", interactive=False)

            collections_dropdown = gr.Dropdown(
                label="é€‰æ‹©çŸ¥è¯†åº“é›†åˆ",
                choices=doc_bases,
                multiselect=True,
                value=None,
                interactive=True
            )

        with gr.Column():
            chat = gr.ChatInterface(
                kunlunrag_thinking_chat,
                title="Thinking LLM Chat Interface ğŸ¤”",
                type="messages",
            )


    # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
    def on_file_select(file):
        """å¤„ç†æ–‡ä»¶é€‰æ‹©"""
        if file is None:
            return "æœªé€‰æ‹©æ–‡ä»¶", ""

        print(f"å·²é€‰æ‹©æ–‡ä»¶: {type(file)}")
        if isinstance(file, str):  # filepathæ¨¡å¼è¿”å›å­—ç¬¦ä¸²è·¯å¾„
            logger.info(f"æ–‡ä»¶å: {os.path.basename(file)}, è·¯å¾„:{os.path.splitext(file)[1]}")

            return os.path.basename(file), os.path.splitext(file)[1]
        elif hasattr(file, 'name'):
            logger.info(f"æ–‡ä»¶å: {file.name}, è·¯å¾„:{os.path.splitext(file.name)[1]}")

            return file.name, os.path.splitext(file.name)[1]
        else:
            return "å·²é€‰æ‹©æ–‡ä»¶ï¼ˆæœªçŸ¥æ ¼å¼ï¼‰", ".txt"


    files_input.change(
        fn=on_file_select,
        inputs=[files_input],
        outputs=[file_name, upload_output]
    )

    # å¤„ç†ä¸Šä¼ æ–‡ä»¶æŒ‰é’®ç‚¹å‡»
    upload_button.click(
        fn=process_uploaded_file,
        inputs=[files_input, dir_input, collection_choice],
        outputs=[upload_output, gr.State(), gr.State()]
    )

    add_collection_btn.click(
        fn=add_new_collection,
        inputs=new_collection_input,
        outputs=[collection_choice, collections_dropdown]
    )


# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    # å¯åŠ¨åº”ç”¨
    demo.launch(server_name="0.0.0.0", share=True)
    # demo.launch(server_name="175.6.21.222", share=True)